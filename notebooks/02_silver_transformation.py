# Databricks notebook source
# MAGIC %md
# MAGIC # ðŸ¥ˆ Capa Silver: TransformaciÃ³n y Limpieza
# MAGIC 
# MAGIC ## Objetivo
# MAGIC Aplicar transformaciones de limpieza y calidad de datos:
# MAGIC - Eliminar duplicados
# MAGIC - Manejar valores nulos
# MAGIC - Normalizar nombres de columnas
# MAGIC - Filtrar registros invÃ¡lidos
# MAGIC - Guardar en formato Delta Lake

# COMMAND ----------

# MAGIC %md
# MAGIC ## Imports y ConfiguraciÃ³n

# COMMAND ----------

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from datetime import datetime

# COMMAND ----------

# MAGIC %md
# MAGIC ## Definir Paths y ConfiguraciÃ³n ADLS Gen2

# COMMAND ----------

# ========================================
# CONFIGURACIÃ“N DEL STORAGE ACCOUNT
# ========================================

# âš ï¸ IMPORTANTE: Actualiza estos valores
storage_account_name = "adlsdatahack90"  # Tu Storage Account

# ----------------------------------------
# Credenciales de Service Principal
# ----------------------------------------
service_principal_client_id = "tuClientId"        # Application (client) ID
service_principal_client_secret = "tuClientSecret"  # Client Secret
service_principal_tenant_id = "tuTenantId"        # Directory (tenant) ID

# COMMAND ----------

# ========================================
# CONFIGURAR AUTENTICACIÃ“N OAUTH 2.0
# ========================================

def configure_service_principal():
    """Configura acceso a ADLS Gen2 con Service Principal"""
    try:
        spark.conf.set(
            f"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net",
            "OAuth"
        )
        spark.conf.set(
            f"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net",
            "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"
        )
        spark.conf.set(
            f"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net",
            service_principal_client_id
        )
        spark.conf.set(
            f"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net",
            service_principal_client_secret
        )
        spark.conf.set(
            f"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net",
            f"https://login.microsoftonline.com/{service_principal_tenant_id}/oauth2/token"
        )
        print(f"ðŸ” Service Principal configurado para: {storage_account_name}")
        return True
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False

# Ejecutar configuraciÃ³n
configure_service_principal()

# COMMAND ----------

# ========================================
# DEFINIR PATHS ADLS GEN2
# ========================================

# Paths del proyecto usando ADLS Gen2 (abfss://)
def get_adls_path(container: str, path: str = "") -> str:
    """Genera la ruta ADLS Gen2 completa"""
    base = f"abfss://{container}@{storage_account_name}.dfs.core.windows.net"
    return f"{base}/{path}" if path else base

BRONZE_PATH = get_adls_path("bronze", "online_retail")
SILVER_PATH = get_adls_path("silver", "online_retail")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Leer Datos de Bronze

# COMMAND ----------

# Leer datos de la capa Bronze
df_bronze = spark.read.parquet(BRONZE_PATH)

print(f"âœ… Datos leÃ­dos de Bronze: {df_bronze.count():,} registros")
print(f"ðŸ“‹ Columnas: {df_bronze.columns}")

# COMMAND ----------

# Vista previa
display(df_bronze.limit(10))

# COMMAND ----------

# MAGIC %md
# MAGIC ## AnÃ¡lisis de Calidad de Datos

# COMMAND ----------

# AnÃ¡lisis de nulos
print("ðŸ” AnÃ¡lisis de Valores Nulos:")
print("-" * 40)

total_rows = df_bronze.count()
for col_name in df_bronze.columns:
    null_count = df_bronze.filter(col(col_name).isNull()).count()
    pct = (null_count / total_rows) * 100
    if null_count > 0:
        print(f"  {col_name}: {null_count:,} ({pct:.2f}%)")

# COMMAND ----------

# AnÃ¡lisis de duplicados
print("ðŸ” AnÃ¡lisis de Duplicados:")
duplicates = df_bronze.count() - df_bronze.dropDuplicates().count()
print(f"  Registros duplicados: {duplicates:,}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Transformaciones de Limpieza

# COMMAND ----------

# MAGIC %md
# MAGIC ### 1. Normalizar Nombres de Columnas

# COMMAND ----------

def normalize_column_name(name: str) -> str:
    """Normaliza nombres de columnas: snake_case, sin espacios"""
    return (name
        .lower()
        .replace(" ", "_")
        .replace("-", "_")
        .replace(".", "_")
    )

# Aplicar normalizaciÃ³n
df_normalized = df_bronze
for old_name in df_bronze.columns:
    new_name = normalize_column_name(old_name)
    df_normalized = df_normalized.withColumnRenamed(old_name, new_name)

print("âœ… Columnas normalizadas:")
print(f"  {df_normalized.columns}")

# COMMAND ----------

# MAGIC %md
# MAGIC ### 2. Filtrar Registros InvÃ¡lidos

# COMMAND ----------

# Filtrar registros con cantidad <= 0 o precio <= 0 (devoluciones y errores)
df_valid = (df_normalized
    .filter(col("quantity") > 0)
    .filter(col("price") > 0)
)

removed_count = df_normalized.count() - df_valid.count()
print(f"âœ… Registros eliminados (cantidad/precio invÃ¡lidos): {removed_count:,}")
print(f"  Registros restantes: {df_valid.count():,}")

# COMMAND ----------

# MAGIC %md
# MAGIC ### 3. Manejar Valores Nulos

# COMMAND ----------

# AnÃ¡lisis de Customer ID nulos
null_customers = df_valid.filter(col("customer_id").isNull()).count()
print(f"ðŸ“Š Registros sin Customer ID: {null_customers:,}")

# Para este proyecto, mantendremos los registros sin customer_id
# pero los marcaremos como "ANONYMOUS"
df_cleaned = (df_valid
    .withColumn("customer_id", 
        when(col("customer_id").isNull(), lit(-1))
        .otherwise(col("customer_id").cast("integer"))
    )
    .withColumn("description",
        when(col("description").isNull(), lit("NO DESCRIPTION"))
        .otherwise(col("description"))
    )
)

print("âœ… Valores nulos manejados")

# COMMAND ----------

# MAGIC %md
# MAGIC ### 4. Eliminar Duplicados

# COMMAND ----------

# Eliminar duplicados exactos
df_deduplicated = df_cleaned.dropDuplicates()

duplicates_removed = df_cleaned.count() - df_deduplicated.count()
print(f"âœ… Duplicados eliminados: {duplicates_removed:,}")

# COMMAND ----------

# MAGIC %md
# MAGIC ### 5. Agregar Columnas Calculadas

# COMMAND ----------

# Agregar columnas Ãºtiles para anÃ¡lisis
df_enriched = (df_deduplicated
    # Total de lÃ­nea
    .withColumn("line_total", round(col("quantity") * col("price"), 2))
    
    # Extraer componentes de fecha
    .withColumn("invoice_year", year(col("invoicedate")))
    .withColumn("invoice_month", month(col("invoicedate")))
    .withColumn("invoice_day", dayofmonth(col("invoicedate")))
    .withColumn("invoice_hour", hour(col("invoicedate")))
    .withColumn("day_of_week", dayofweek(col("invoicedate")))
    
    # Indicador de transacciÃ³n cancelada (Invoice empieza con 'C')
    .withColumn("is_cancelled", 
        when(col("invoice").startswith("C"), lit(True))
        .otherwise(lit(False))
    )
    
    # Timestamp de procesamiento Silver
    .withColumn("_silver_timestamp", current_timestamp())
)

print("âœ… Columnas calculadas agregadas:")
new_cols = [c for c in df_enriched.columns if c not in df_deduplicated.columns]
print(f"  {new_cols}")

# COMMAND ----------

# Vista previa del resultado
display(df_enriched.limit(10))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Guardar en Capa Silver (Delta Lake)

# COMMAND ----------

# Guardar en formato Delta con particiÃ³n por aÃ±o y mes
(df_enriched
    .write
    .format("delta")
    .mode("overwrite")
    .partitionBy("invoice_year", "invoice_month")
    .save(SILVER_PATH)
)

print(f"âœ… Datos guardados en: {SILVER_PATH}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Optimizar Tabla Delta

# COMMAND ----------

# Optimizar la tabla Delta para mejor performance
spark.sql(f"OPTIMIZE delta.`{SILVER_PATH}`")
print("âœ… Tabla Delta optimizada")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Verificar Escritura

# COMMAND ----------

# Verificar datos guardados
df_verify = spark.read.format("delta").load(SILVER_PATH)
print(f"âœ… VerificaciÃ³n: {df_verify.count():,} registros en Silver")

# COMMAND ----------

# Ver historial de versiones Delta
display(spark.sql(f"DESCRIBE HISTORY delta.`{SILVER_PATH}`"))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Registrar como Vista/Tabla (Compatible con Unity Catalog)
# MAGIC 
# MAGIC **Nota**: DBFS pÃºblico estÃ¡ deshabilitado en workspaces modernos.

# COMMAND ----------

# ========================================
# OPCIÃ“N 1: Crear Vista Temporal
# ========================================

df_verify = spark.read.format("delta").load(SILVER_PATH)
df_verify.createOrReplaceTempView("silver_online_retail")

print("âœ… Vista temporal 'silver_online_retail' creada")
print("   Puedes usar: SELECT * FROM silver_online_retail")

# COMMAND ----------

# ========================================
# OPCIÃ“N 2: Unity Catalog (Si estÃ¡ habilitado)
# ========================================
# Descomenta si tienes Unity Catalog

"""
spark.sql("CREATE SCHEMA IF NOT EXISTS tu_catalogo.retail_medallion")
spark.sql(f'''
    CREATE TABLE IF NOT EXISTS tu_catalogo.retail_medallion.silver_online_retail
    USING DELTA
    LOCATION '{SILVER_PATH}'
''')
print("âœ… Tabla Unity Catalog creada")
"""

# COMMAND ----------

# Query de prueba usando vista temporal
print("ðŸ“Š Top 10 paÃ­ses por revenue:")
display(spark.sql("""
    SELECT 
        country,
        COUNT(*) as transactions,
        ROUND(SUM(line_total), 2) as total_revenue,
        COUNT(DISTINCT customer_id) as unique_customers
    FROM silver_online_retail
    WHERE is_cancelled = false
    GROUP BY country
    ORDER BY total_revenue DESC
    LIMIT 10
"""))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Resumen de la Capa Silver

# COMMAND ----------

# ComparaciÃ³n Bronze vs Silver
bronze_count = df_bronze.count()
silver_count = df_enriched.count()
reduction = bronze_count - silver_count
reduction_pct = (reduction / bronze_count) * 100

summary = {
    "Capa": "Silver",
    "Formato Entrada": "Parquet (Bronze)",
    "Formato Salida": "Delta Lake",
    "Registros Bronze": f"{bronze_count:,}",
    "Registros Silver": f"{silver_count:,}",
    "ReducciÃ³n": f"{reduction:,} ({reduction_pct:.1f}%)",
    "Columnas Nuevas": len(df_enriched.columns) - len(df_bronze.columns),
    "Path": SILVER_PATH,
    "Timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
}

print("=" * 60)
print("ðŸ“Š RESUMEN CAPA SILVER")
print("=" * 60)
for key, value in summary.items():
    print(f"  {key}: {value}")
print("=" * 60)

# COMMAND ----------

# Transformaciones aplicadas
print("\nðŸ”§ Transformaciones Aplicadas:")
print("  1. âœ… NormalizaciÃ³n de nombres de columnas")
print("  2. âœ… Filtrado de cantidad/precio invÃ¡lidos")
print("  3. âœ… Manejo de valores nulos")
print("  4. âœ… EliminaciÃ³n de duplicados")
print("  5. âœ… Columnas calculadas (line_total, fechas, is_cancelled)")
print("  6. âœ… ConversiÃ³n a formato Delta Lake")
print("  7. âœ… Particionamiento por aÃ±o/mes")

# COMMAND ----------

# MAGIC %md
# MAGIC ---
# MAGIC ## âœ… PrÃ³ximo Paso
# MAGIC 
# MAGIC ContinÃºa con el notebook **03_gold_aggregation.py** para crear las agregaciones de negocio.
